{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    As everybody is aware, the past Administration...\n",
       "1    “This isn’t some game. You are screwing with t...\n",
       "2    I have been briefed on the U.S. C-130 “Hercule...\n",
       "3    Congratulations @SecPompeo! https://t.co/ECrMG...\n",
       "4    A Rigged System - They don’t want to turn over...\n",
       "Name: full_text, dtype: object"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "df = pd.read_json('realDonaldTrump.jsonl', lines=True)\n",
    "df = df['full_text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(['https'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use either TfidfVectorizer or HashingVectorizer, but since HashingVectorizer aims on low memory usage and it cannot compute the inverse transform(requires a pipeline) we choose TfidfVectorizer instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words,token_pattern = r'\\b[a-zA-Z]{3,}\\b',lowercase = True)\n",
    "X = vectorizer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our data is small and we are not trying to reduce the computation time we will be using k-means instead of miniBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=10, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=10,init='k-means++', max_iter=100, n_init=1 )\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient: 0.003\n"
     ]
    }
   ],
   "source": [
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette score computes the compactness of a cluster, where higher is better, this will depend on the number of clusters we are targeting. But overall it seems that is doing a decent job trying to cluster the terms. Cluster 2 is an obvious one, top term is \"new\" and it contains words such as \"fake, great, media\". This makes sense becase he is always talking about \"fake news\" , \"make america great again\" and the media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0: trump\n",
      " minister\n",
      " prime\n",
      " president\n",
      " abeshinzo\n",
      " congratulations\n",
      " donald\n",
      " shows\n",
      " happens\n",
      " nikkihaley\n",
      "Cluster 1: whitehouse\n",
      " join\n",
      " america\n",
      " great\n",
      " making\n",
      " flotus\n",
      " maga\n",
      " bush\n",
      " president\n",
      " michigan\n",
      "Cluster 2: news\n",
      " fake\n",
      " great\n",
      " media\n",
      " foxandfriends\n",
      " doing\n",
      " believe\n",
      " spending\n",
      " cnn\n",
      " just\n",
      "Cluster 3: korea\n",
      " north\n",
      " just\n",
      " meeting\n",
      " kim\n",
      " time\n",
      " nuclear\n",
      " south\n",
      " jong\n",
      " good\n",
      "Cluster 4: border\n",
      " wall\n",
      " laws\n",
      " democrats\n",
      " country\n",
      " people\n",
      " daca\n",
      " mexico\n",
      " congress\n",
      " southern\n",
      "Cluster 5: russia\n",
      " collusion\n",
      " tremendous\n",
      " long\n",
      " office\n",
      " lawyers\n",
      " amp\n",
      " investigation\n",
      " election\n",
      " campaign\n",
      "Cluster 6: great\n",
      " honor\n",
      " maga\n",
      " thank\n",
      " today\n",
      " people\n",
      " whitehouse\n",
      " welcome\n",
      " american\n",
      " work\n",
      "Cluster 7: hunt\n",
      " witch\n",
      " crime\n",
      " justice\n",
      " collusion\n",
      " total\n",
      " hard\n",
      " happened\n",
      " department\n",
      " obstruction\n",
      "Cluster 8: jobs\n",
      " comey\n",
      " hillary\n",
      " james\n",
      " mccabe\n",
      " history\n",
      " fbi\n",
      " lied\n",
      " clinton\n",
      " crooked\n",
      "Cluster 9: trade\n",
      " states\n",
      " united\n",
      " happy\n",
      " years\n",
      " china\n",
      " countries\n",
      " war\n",
      " end\n",
      " treated\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(10):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='',)\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
